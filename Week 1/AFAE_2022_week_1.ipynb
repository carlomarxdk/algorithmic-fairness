{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Gender bias in word embedding\n",
    "\n",
    "In a word embedding, a word is represented as a d-dimensional vector of real numbers. In [this paper](https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf) (discussed in class), Bolukbasi & co-authors study gender bias associated with occupations in a word2vec embedding and found two lists of occupations with an extreme gender bias - see the two lists below. These lists have been identified by comparing the similarity of the vector of each occupation, say _nurse_, with the vectors for the words _he_ and _she_. Whenever an occupation was closer to he (or she) than a given threshold, the occupation was included in the \"extreme he\" (\"extreme she\") occupations. In this case _similarity_ was quantified as the inner product of two sets of vectors.\n",
    "\n",
    "Extreme \"she\" occupations:\n",
    "\n",
    "        ['homemaker' \n",
    "        'nurse'\n",
    "        'receptionist'\n",
    "        'librarian'\n",
    "        'socialite'\n",
    "        'hairdresser'\n",
    "        'nanny'\n",
    "        'bookkeeper'\n",
    "        'stylist'\n",
    "        'housekeeper'\n",
    "        'interior designer'\n",
    "        'guidance counselor']\n",
    "\n",
    "\n",
    "Extreme \"he\" occupations:\n",
    "\n",
    "        ['maestro'\n",
    "        'skipper'\n",
    "        'protege'\n",
    "        'philosopher'\n",
    "        'captain'\n",
    "        'architect'\n",
    "        'financier'\n",
    "        'warrior'\n",
    "        'broadcaster'\n",
    "        'magician'\n",
    "        'fighter pilot'\n",
    "        'boss']\n",
    "\n",
    "The embeddings used in the paper is a word2vec, trained on a 3 million word corpus of Google News. We provide you with a different word embedding, done with [GloVe](https://nlp.stanford.edu/projects/glove/) and trained on the Wikipedia 2014 + Gigaword 5th Edition corpora. These combined corpora are made up of 6 billion words, an is two thousand times bigger than the corpus used to train the word2vec embeddings.\n",
    "\n",
    "Given the differences between this embedding and the one of the original paper, we ask: \n",
    "do these same biased relationships exist between the words \"he\" and \"she\" and these occupations?\n",
    "\n",
    "Do the following:\n",
    "* Load the Glove word embedding data set (glove.6B.50d.txt)\n",
    "* Calculate the similarity between the two lists of occupations above and the words _he_ and _she_\n",
    "* Check if bias exists, that is if the occupations in the \"extreme he\" list are also biased tow he extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
